{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "automl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNwiQxsz1IQd"
      },
      "source": [
        "**Automated ML**\r\n",
        "\r\n",
        "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMcCv4bM04fR"
      },
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "import csv\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn import datasets\r\n",
        "import pkg_resources\r\n",
        "\r\n",
        "import azureml.core\r\n",
        "from azureml.core.experiment import Experiment\r\n",
        "from azureml.core.workspace import Workspace\r\n",
        "from azureml.train.automl import AutoMLConfig\r\n",
        "from azureml.core.dataset import Dataset\r\n",
        "\r\n",
        "from azureml.pipeline.steps import AutoMLStep\r\n",
        "\r\n",
        "# Check core SDK version number\r\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRBV9sMs1ETX"
      },
      "source": [
        "**Dataset**\r\n",
        "\r\n",
        "***Overview***\r\n",
        "\r\n",
        "TODO: In this markdown cell, give an overview of the dataset you are using. Also mention the task you will be performing.\r\n",
        "\r\n",
        "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43zx6FqC1DMH"
      },
      "source": [
        "ws = Workspace.from_config()\r\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rU4w8tQ1TsF"
      },
      "source": [
        "**Create or Attach an AmlCompute clusterÂ¶**\r\n",
        "\r\n",
        "You will need to create a compute target for your AutoML run. In this tutorial, you get the default AmlCompute as your training compute resource.\r\n",
        "\r\n",
        "Udacity Note There is no need to create a new compute target, it can re-use the previous cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCRP-9Ae1Xvj"
      },
      "source": [
        "from azureml.core.compute import AmlCompute\r\n",
        "from azureml.core.compute import ComputeTarget\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# NOTE: update the cluster name to match the existing cluster\r\n",
        "# Choose a name for your CPU cluster\r\n",
        "amlcompute_cluster_name = \"automl-cluster\"\r\n",
        "\r\n",
        "# Verify that cluster does not exist already\r\n",
        "try:\r\n",
        "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',# for GPU, use \"STANDARD_NC6\"\r\n",
        "                                                           #vm_priority = 'lowpriority', # optional\r\n",
        "                                                           max_nodes=4)\r\n",
        "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\r\n",
        "\r\n",
        "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\r\n",
        "# For a more detailed view of current AmlCompute status, use get_status()."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogGpnyip1kXe"
      },
      "source": [
        "**Data**\r\n",
        "\r\n",
        "**Udacity note:**\r\n",
        "\r\n",
        "Make sure the key is the same name as the dataset that is uploaded, and that the description matches. If it is hard to find or unknown, loop over the ws.datasets.keys() and print() them. If it isn't found because it was deleted, it can be recreated with the link that has the CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7vzLvTO1lXM"
      },
      "source": [
        "# Try to load the dataset from the Workspace. Otherwise, create it from the file\r\n",
        "# NOTE: update the key to match the dataset name\r\n",
        "found = False\r\n",
        "key = \"LV_cleaned_data\"\r\n",
        "description_text = \"LV hotel reviews from trip advisor\"\r\n",
        "\r\n",
        "if key in ws.datasets.keys(): \r\n",
        "        found = True\r\n",
        "        dataset = ws.datasets[key] \r\n",
        "\r\n",
        "if not found:\r\n",
        "        # Create AML Dataset and register it into Workspace\r\n",
        "        LV_cleaned_data = 'https://raw.githubusercontent.com/Kbhamidipa3/udacityazure_capstone_final/main/LasVegasTripAdvisorReviews-Dataset.csv'\r\n",
        "        dataset = Dataset.Tabular.from_delimited_files(LV_cleaned_data)        \r\n",
        "        #Register Dataset in Workspace\r\n",
        "        dataset = dataset.register(workspace=ws,\r\n",
        "                                   name=key,\r\n",
        "                                   description=description_text)\r\n",
        "\r\n",
        "\r\n",
        "df = dataset.to_pandas_dataframe()\r\n",
        "config = ScriptRunConfig(source_directory='training', script='train.py', compute_target='cpu-cluster')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNEdHKEu1o2m"
      },
      "source": [
        "**Review the Dataset Result**\r\n",
        "\r\n",
        "You can peek the result of a TabularDataset at any range using skip(i) and take(j).to_pandas_dataframe(). Doing so evaluates only j records for all the steps in the TabularDataset, which makes it fast even against large datasets.\r\n",
        "\r\n",
        "TabularDataset objects are composed of a list of transformation steps (optional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL7AmF-p1-XL"
      },
      "source": [
        "dataset.take(5).to_pandas_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejL6DaPQ3skj"
      },
      "source": [
        "**Split into train and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaAc9HlC3B1N"
      },
      "source": [
        "from train import clean_data\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        " \r\n",
        "x, y = clean_data(ds)\r\n",
        "train_data, test_data = dataset.random_split(percentage=0.75, seed=223)\r\n",
        "x_train = train_data.pop_columns(columns=['Score'])\r\n",
        "y_train = train_data.Score\r\n",
        "x_test, y_train, y_test \r\n",
        "data_train = pd.concat([x_train,y_train], axis=1)\r\n",
        "\r\n",
        "os.makedirs('data_train', exist_ok=True)\r\n",
        "\r\n",
        "local_path = './data_train/data_train.csv'\r\n",
        "data_train.to_csv(local_path)\r\n",
        "\r\n",
        "# upload the local file to a datastore on the cloud\r\n",
        "workspace = Workspace(ws.subscription_id, ws.resource_group, ws.name)\r\n",
        "\r\n",
        "# get the datastore to upload prepared data\r\n",
        "datastore = ws.get_default_datastore()\r\n",
        "\r\n",
        "# upload the local file from src_dir to the target_path in datastore\r\n",
        "datastore.upload(src_dir='data_train', target_path='data_train')\r\n",
        "\r\n",
        "# create a dataset referencing the cloud location\r\n",
        "data_train = Dataset.Tabular.from_delimited_files(path = [(datastore, ('data_train/data_train.csv'))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaziKBGT2CCm"
      },
      "source": [
        "**AutoML Configuration**\r\n",
        "\r\n",
        "TODO: Explain why you chose the automl settings and cofiguration you used below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4oAWjuj2GeT"
      },
      "source": [
        "automl_settings = {\r\n",
        "    \"experiment_timeout_minutes\": 20,\r\n",
        "    \"max_concurrent_iterations\": 5,\r\n",
        "    \"primary_metric\" : 'accuracy'\r\n",
        "}\r\n",
        "automl_config = AutoMLConfig(compute_target=compute_target,\r\n",
        "                             task = \"classification\",\r\n",
        "                             training_data=dataset,\r\n",
        "                             label_column_name=\"Score\",   \r\n",
        "                             path = project_folder,\r\n",
        "                             enable_early_stopping= True,\r\n",
        "                             featurization= 'auto',\r\n",
        "                             debug_log = \"automl_errors.log\",\r\n",
        "                             **automl_settings\r\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFf0TOXA4GeW"
      },
      "source": [
        "**Submit your automl run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0qZS5T44EQg"
      },
      "source": [
        "experiment_name = 'lv-automl-classification'\r\n",
        "experiment = Experiment(ws, experiment_name)\r\n",
        "run = experiment.submit(automl_config, show_output=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVm7S0Cf5gaY"
      },
      "source": [
        "Results\r\n",
        "Widget for Monitoring Runs\r\n",
        "The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\r\n",
        "\r\n",
        "Note: The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ike1GL_15pdE"
      },
      "source": [
        "from azureml.widgets import RunDetails\r\n",
        "RunDetails(run).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Mgb1o_4OZR"
      },
      "source": [
        "**Analyze results**\r\n",
        "\r\n",
        "Retrieve the Best Model\r\n",
        "Below we select the best pipeline from our iterations. The get_output method on automl_classifier returns the best run and the fitted model for the last invocation. Overloads on get_output allow you to retrieve the best run and fitted model for any logged metric or for a particular iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayaOLXjy4O7F"
      },
      "source": [
        "best_run, fitted_model = run.get_output()\r\n",
        "print(best_run)\r\n",
        "print(fitted_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KE7lJDI6AQI"
      },
      "source": [
        "**Tests**\r\n",
        "\r\n",
        "Now that the model is trained, split the data in the same way the data was split for training (The difference here is the data is being split locally) and then run the test data through the trained model to get the predicted values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7NePAUs6Ed4"
      },
      "source": [
        "# convert the test data to dataframe\r\n",
        "X_test_df = test_data.pop(columns=[label_column_name]).to_pandas_dataframe()\r\n",
        "y_test_df = test_data.Score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkzH8HHv6c0Q"
      },
      "source": [
        "# call the predict functions on the model\r\n",
        "y_pred = fitted_model.predict(X_test_df)\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LHtTWaK6f_P"
      },
      "source": [
        "**Calculate metrics for the prediction**\r\n",
        "\r\n",
        "Now visualize the data on a scatter plot to show what our truth (actual) values are compared to the predicted values from the trained model that was returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEVPEt2R6jtY"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "import numpy as np\r\n",
        "import itertools\r\n",
        "\r\n",
        "cf =confusion_matrix(y_test_df.values,y_pred)\r\n",
        "plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\r\n",
        "plt.colorbar()\r\n",
        "plt.title('Confusion Matrix')\r\n",
        "plt.xlabel('Predicted')\r\n",
        "plt.ylabel('Actual')\r\n",
        "class_labels = ['False','True']\r\n",
        "tick_marks = np.arange(len(class_labels))\r\n",
        "plt.xticks(tick_marks,class_labels)\r\n",
        "plt.yticks([-0.5,0,1,1.5],['','False','True',''])\r\n",
        "# plotting text value inside cells\r\n",
        "thresh = cf.max() / 2.\r\n",
        "for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\r\n",
        "    plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jcRHaNk6zlG"
      },
      "source": [
        "**Run the explanation**\r\n",
        "\r\n",
        "Download the engineered feature importance from artifact store\r\n",
        "You can use ExplanationClient to download the engineered feature explanations from the artifact store of the best_run. You can also use azure portal url to view the dash board visualization of the feature importance values of the engineered features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdFNkPhG63KK"
      },
      "source": [
        "client = ExplanationClient.from_run(best_run)\r\n",
        "engineered_explanations = client.download_model_explanation(raw=False)\r\n",
        "print(engineered_explanations.get_feature_importance_dict())\r\n",
        "print(\"You can visualize the engineered explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + best_run.get_portal_url())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXg3mvYG6_Q3"
      },
      "source": [
        "**Download the raw feature importance from artifact store**\r\n",
        "\r\n",
        "You can use ExplanationClient to download the raw feature explanations from the artifact store of the best_run. You can also use azure portal url to view the dash board visualization of the feature importance values of the raw features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DoDDylC7DVL"
      },
      "source": [
        "raw_explanations = client.download_model_explanation(raw=True)\r\n",
        "print(raw_explanations.get_feature_importance_dict())\r\n",
        "print(\"You can visualize the raw explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + best_run.get_portal_url())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQZlrP9H7JGx"
      },
      "source": [
        "**Retrieve any other AutoML model from training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxR5IT5-7LUs"
      },
      "source": [
        "automl_run, fitted_model = run.get_output(metric='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkRyAGQ7eGx"
      },
      "source": [
        "**Setup the model explanations for AutoML models**\r\n",
        "\r\n",
        "The fitted_model can generate the following which will be used for getting the engineered explanations using automl_setup_model_explanations:-\r\n",
        "\r\n",
        "Featurized data from train samples/test samples\r\n",
        "Gather engineered name lists\r\n",
        "Find the classes in your labeled column in classification scenarios\r\n",
        "The automl_explainer_setup_obj contains all the structures from above list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb8TPM4T7gw6"
      },
      "source": [
        "X_train = training_data.drop_columns(columns=[label_column_name])\r\n",
        "y_train = training_data.keep_columns(columns=[label_column_name], validate=True)\r\n",
        "X_test = validation_data.drop_columns(columns=[label_column_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm6kUwjq7kq4"
      },
      "source": [
        "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\r\n",
        "\r\n",
        "automl_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X=X_train, \r\n",
        "                                                             X_test=X_test, y=y_train, \r\n",
        "                                                             task='classification')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdO6qFFN8I7H"
      },
      "source": [
        "**Initialize the Mimic Explainer for feature importance**\r\n",
        "\r\n",
        "For explaining the AutoML models, use the MimicWrapper from azureml-interpret package. The MimicWrapper can be initialized with fields in automl_explainer_setup_obj, your workspace and a surrogate model to explain the AutoML model (fitted_model here). The MimicWrapper also takes the automl_run object where engineered explanations will be uploaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIh02E1O8NaH"
      },
      "source": [
        "from interpret.ext.glassbox import LGBMExplainableModel\r\n",
        "from azureml.interpret.mimic_wrapper import MimicWrapper\r\n",
        "explainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator,\r\n",
        "                         explainable_model=automl_explainer_setup_obj.surrogate_model, \r\n",
        "                         init_dataset=automl_explainer_setup_obj.X_transform, run=automl_run,\r\n",
        "                         features=automl_explainer_setup_obj.engineered_feature_names, \r\n",
        "                         feature_maps=[automl_explainer_setup_obj.feature_map],\r\n",
        "                         classes=automl_explainer_setup_obj.classes,\r\n",
        "                         explainer_kwargs=automl_explainer_setup_obj.surrogate_model_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgseHnEx8T6X"
      },
      "source": [
        "**Use Mimic Explainer for computing and visualizing engineered feature importance**\r\n",
        "\r\n",
        "The explain() method in MimicWrapper can be called with the transformed test samples to get the feature importance for the generated engineered features. You can also use azure portal url to view the dash board visualization of the feature importance values of the engineered features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lFGepbf8ZP3"
      },
      "source": [
        "# Compute the engineered explanations\r\n",
        "engineered_explanations = explainer.explain(['local', 'global'], eval_dataset=automl_explainer_setup_obj.X_test_transform)\r\n",
        "print(engineered_explanations.get_feature_importance_dict())\r\n",
        "print(\"You can visualize the engineered explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + automl_run.get_portal_url())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOFkPVia8eg0"
      },
      "source": [
        "**Use Mimic Explainer for computing and visualizing raw feature importance**\r\n",
        "\r\n",
        "The explain() method in MimicWrapper can be called with the transformed test samples to get the feature importance for the original features in your data. You can also use azure portal url to view the dash board visualization of the feature importance values of the original/raw features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVpMicFy8mHW"
      },
      "source": [
        "# Compute the raw explanations\r\n",
        "raw_explanations = explainer.explain(['local', 'global'], get_raw=True,\r\n",
        "                                     raw_feature_names=automl_explainer_setup_obj.raw_feature_names,\r\n",
        "                                     eval_dataset=automl_explainer_setup_obj.X_test_transform,\r\n",
        "                                     raw_eval_dataset=automl_explainer_setup_obj.X_test_raw)\r\n",
        "print(raw_explanations.get_feature_importance_dict())\r\n",
        "print(\"You can visualize the raw explanations under the 'Explanations (preview)' tab in the AutoML run at:-\\n\" + automl_run.get_portal_url())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoRVRw9P8n02"
      },
      "source": [
        "**Initialize the scoring Explainer, save and upload it for later use in scoring explanation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C31ztX008s86"
      },
      "source": [
        "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer\r\n",
        "import joblib\r\n",
        "\r\n",
        "# Initialize the ScoringExplainer\r\n",
        "scoring_explainer = TreeScoringExplainer(explainer.explainer, feature_maps=[automl_explainer_setup_obj.feature_map])\r\n",
        "\r\n",
        "# Pickle scoring explainer locally to './scoring_explainer.pkl'\r\n",
        "scoring_explainer_file_name = 'scoring_explainer.pkl'\r\n",
        "with open(scoring_explainer_file_name, 'wb') as stream:\r\n",
        "    joblib.dump(scoring_explainer, stream)\r\n",
        "\r\n",
        "# Register trained automl model present in the 'outputs' folder in the artifacts\r\n",
        "original_model = automl_run.register_model(model_name='automl_model', \r\n",
        "                                           model_path='outputs/model.pkl')\r\n",
        "\r\n",
        "# Upload the scoring explainer to the automl run\r\n",
        "automl_run.upload_file('outputs/scoring_explainer.pkl', scoring_explainer_file_name)\r\n",
        "scoring_explainer_model = automl_run.register_model(model_name='scoring_explainer', model_path='scoring_explainer.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVO-goYF-AAe"
      },
      "source": [
        "**Create the conda dependencies for setting up the service**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGkwlXC-Bgj"
      },
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "\r\n",
        "azureml_pip_packages = [\r\n",
        "    'azureml-interpret', 'azureml-train-automl', 'azureml-defaults'\r\n",
        "]\r\n",
        "\r\n",
        "myenv = CondaDependencies.create(conda_packages=['scikit-learn', 'pandas', 'numpy', 'py-xgboost<=0.80'],\r\n",
        "                                 pip_packages=azureml_pip_packages,\r\n",
        "                                 pin_sdk_version=True)\r\n",
        "\r\n",
        "with open(\"myenv.yml\",\"w\") as f:\r\n",
        "    f.write(myenv.serialize_to_string())\r\n",
        "\r\n",
        "with open(\"myenv.yml\",\"r\") as f:\r\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxklb5ZD-Dv3"
      },
      "source": [
        "**Deploy the service**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqoIOtvLHvnQ"
      },
      "source": [
        "from azureml.core.webservice import Webservice\r\n",
        "from azureml.core.webservice import AciWebservice\r\n",
        "from azureml.core.model import Model, InferenceConfig\r\n",
        "from azureml.core.environment import Environment\r\n",
        "\r\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,\r\n",
        "                                               memory_gb=1,\r\n",
        "                                               tags={\"data\": \"LV_cleaned_data\",  \r\n",
        "                                                     \"method\" : \"local_explanation\"},\r\n",
        "                                               description='Get local explanations for Bank marketing test data')\r\n",
        "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\r\n",
        "inference_config = InferenceConfig(entry_script=\"score_local_explain.py\", environment=myenv)\r\n",
        "\r\n",
        "# Use configs and models generated above\r\n",
        "service = Model.deploy(ws,\r\n",
        "                       'model-scoring',\r\n",
        "                       [scoring_explainer_model, original_model],\r\n",
        "                       inference_config,\r\n",
        "                       aciconfig)\r\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddbCXV7QIA2w"
      },
      "source": [
        "**View the service logs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxkOakJ2I6F3"
      },
      "source": [
        "service.get_logs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj2gflNVH6Oh"
      },
      "source": [
        "**Inference with test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucn5YGRUH982"
      },
      "source": [
        "if service.state == 'Healthy':\r\n",
        "    # Serialize the first row of the test data into json\r\n",
        "    X_test_json = X_test[:1].to_json(orient='records')\r\n",
        "    print(X_test_json)\r\n",
        "    # Call the service to get the predictions and the engineered explanations\r\n",
        "    output = service.run(X_test_json)\r\n",
        "    # Print the predicted value\r\n",
        "    print(output['predictions'])\r\n",
        "    # Print the engineered feature importances for the predicted value\r\n",
        "    print(output['engineered_local_importance_values'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEXEbXzbJLh_"
      },
      "source": [
        "**Delete the service.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1uXmUpOJNET"
      },
      "source": [
        "service.delete()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J3kcdv84ae6"
      },
      "source": [
        "**Delete the cluster at the end of the run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL71J_4z4ZN_"
      },
      "source": [
        "cpu_cluster.delete()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}